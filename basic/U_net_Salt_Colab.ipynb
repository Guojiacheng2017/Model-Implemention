{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "n_fold = 5  # ?\n",
    "pad_left = 0\n",
    "pad_right = 0\n",
    "fine_size = 202 # ?\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "snapshot = 6 # ? what for\n",
    "max_lr = 0.012\n",
    "min_lr = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "save_weight = '../rst/'\n",
    "\n",
    "# if not os.path.isdir(save_weight):\n",
    "#     os.makedirs(save_weight)\n",
    "weight_name = 'model_' + str(fine_size + pad_left + pad_right)\n",
    "\n",
    "train_image_path = './data/SaltDataset/train/images'\n",
    "train_mask_path = './data/SaltDataset/train/masks'\n",
    "test_image_path = './data/SaltDataset/test/images'\n",
    "train_file = './data/SaltDataset/train.csv'\n",
    "depths_file = './data/SaltDataset/depths.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Image --[GAN get latent]--> use the depth --[get feature map latent]-->"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.encoder(img)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.img_shape = (((img_shape // 2) // 2) // 2) // 2\n",
    "        self.linear = nn.Linear(self.img_shape ^2 * 256, 128)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        rst = self.conv(img)\n",
    "        rst = rst.view(img.shape[0], -1)\n",
    "        rst = self.linear(rst)\n",
    "        rst = self.fc(rst)\n",
    "        return rst"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "4000"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depths = pd.read_csv('./data/SaltDataset/depths.csv')\n",
    "# depths.sort_values('z', inplace=True)\n",
    "# depths.drop('z', axis=1, inplace=True)\n",
    "# depths['fold'] = (list(range(0,5)) * depths.shape[0])[:depths.shape[0]]\n",
    "#\n",
    "train_df = pd.read_csv('./data/SaltDataset/train.csv')\n",
    "len(train_df)\n",
    "# train_df = train_df.merge(depths)\n",
    "# dist = []\n",
    "# for id in train_df.id.values:\n",
    "#   img = cv2.imread(f'./data/SaltDataset/train/images/{id}.png', cv2.IMREAD_GRAYSCALE)\n",
    "#   dist.append(np.unique(img).shape[0])\n",
    "# train_df['unique_pixels'] = dist\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SaltDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, train_file, depth_file, img_size=224, mode='train', transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.train_df = pd.read_csv(train_file).drop('rle_mask', axis=1)\n",
    "        self.depth_df = pd.read_csv(depth_file)\n",
    "        self.train_df = self.train_df.merge(self.depth_df)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        # self.mode = mode\n",
    "        # if mode == 'train':\n",
    "        #     self.file_idx = list(range(len(self.train_df)))\n",
    "        #     self.train_idx, self.val_idx = train_test_split(file_idx, test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # if self.train_idx:\n",
    "        #     pass\n",
    "        # elif self.mode == 'val':\n",
    "        #     pass\n",
    "        img_path = os.path.join(self.img_path, self.train_df.values[idx, 0] + '.png')\n",
    "        mask_path = os.path.join(self.mask_path, self.train_df.values[idx, 0] + '.png')\n",
    "        depth = self.train_df.values[idx, 1]\n",
    "\n",
    "        # image = read_image(img_path) # , mode=ImageReadMode.GRAY\n",
    "        # mask = read_image(mask_path)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "\n",
    "        image = np.resize(image, (self.img_size, self.img_size))\n",
    "        mask = np.resize(mask, (self.img_size, self.img_size))\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "            # depth = self.transform(depth)\n",
    "\n",
    "        return image, mask, depth\n",
    "\n",
    "\n",
    "# def trainImageFetch(image_len):\n",
    "#     image_train = np.zeros((image_len, 101, 101), dtype=np.float32) / 255\n",
    "#     mask_train = np.zeros((image_len, 101, 101), dtype=np.float32) / 255\n",
    "\n",
    "# depths = pd.read_csv('./data/SaltDataset/depths.csv')\n",
    "# train_df = pd.read_csv('./data/SaltDataset/train.csv')\n",
    "# train_df = train_df.merge(depths)       # merge 就像数据库一样合并表，主键自动匹配\n",
    "\n",
    "# for id in train_df.id.values:\n",
    "#     img = cv2.imread(f'./data/SaltDataset/train/images/{id}.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     dist.append(np.unique(img).shape[0])\n",
    "#     # print(img.shape)\n",
    "#\n",
    "#\n",
    "# train_df['unique_pixels'] = dist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataset = SaltDataset(train_image_path, train_mask_path, train_file, depths_file, transform=transforms.ToTensor())\n",
    "\n",
    "file_idx = list(range(train_df.shape[0]))\n",
    "train_idx, val_idx = train_test_split(file_idx, test_size=0.2, shuffle=True)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "# print(training_data.__dict__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# model\n",
    "g_model = Generator().to(device)\n",
    "d_model = Discriminator(img_shape=101).to(device)\n",
    "\n",
    "# optimzer\n",
    "G_optim = optim.SGD(g_model.parameters(), lr=min_lr, momentum=0.05)\n",
    "D_optim = optim.SGD(d_model.parameters(), lr=min_lr, momentum=0.05)\n",
    "\n",
    "# loss\n",
    "g_loss = nn.MSELoss()\n",
    "d_loss = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### U-net"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from U_net import Unet\n",
    "\n",
    "model = Unet(1, pad=True).to(device)\n",
    "U_optim = optim.SGD(model.parameters(), lr=min_lr, momentum=0.05)\n",
    "u_loss = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bling/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([64, 1, 224, 224])) that is different to the input size (torch.Size([64, 2, 224, 224])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6277228593826294\n",
      "loss: 0.5458352565765381\n",
      "loss: 0.6515582203865051\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 32\u001B[0m\n\u001B[1;32m     29\u001B[0m loss \u001B[38;5;241m=\u001B[39m u_loss(preds, masks)\n\u001B[1;32m     31\u001B[0m U_optim\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 32\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m U_optim\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# print(train_loader.dataset.__getitem__(0))\n",
    "\n",
    "def train(dataloader, model, epochs):\n",
    "    pass\n",
    "\n",
    "# train discriminator\n",
    "for epoch in range(epochs):\n",
    "    for idx, (imgs, masks, depths) in enumerate(train_loader, start=0):\n",
    "        # print(imgs), print(masks), print(depths)\n",
    "        # imgs = imgs.to(device)\n",
    "        # # masks = masks.to(device)\n",
    "        # pred_feature = g_model(depths)\n",
    "        # pred_depth = d_model(pred_feature)\n",
    "        # loss_a = d_loss(pred_depth, depths)\n",
    "        # loss_b = d_loss(imgs, depths)\n",
    "        # loss = (loss_a + loss_b) / 2\n",
    "        #\n",
    "        # # optimizer.zero\n",
    "        # G_optim.zero_grad()\n",
    "        # D_optim.zero_grad()\n",
    "        #\n",
    "        # #\n",
    "        # loss.backward()\n",
    "        # D_optim.step()\n",
    "        # G_optim.step()\n",
    "        #\n",
    "        # print(f\"Pred_depth: {pred_depth}\\tloss_a: {loss_a}\\tloss_b: {loss_b}\\tTotal Loss: {loss}\")\n",
    "        preds = model(imgs)\n",
    "        loss = u_loss(preds, masks)\n",
    "\n",
    "        U_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        U_optim.step()\n",
    "        print(f\"loss: {loss}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_list = []\n",
    "mask_list = []\n",
    "# for _, id in tqdm(enumerate(train_df.id.values), total=len(train_df)):\n",
    "#     # print(id)\n",
    "#     img = cv2.imread(f'./data/SaltDataset/train/images/{id}.png', cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "#     mask = cv2.imread(f'./data/SaltDataset/train/masks/{id}.png', cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "#     # print(img)\n",
    "#     img_list.append(img)\n",
    "#     mask_list.append(mask)\n",
    "#\n",
    "# print(img_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(mask_list)\n",
    "# print(train_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_df.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "# train_data = np.concatenate((img_list, mask_list), axis=0)\n",
    "# train_data = pd.DataFrame(data=img_list)\n",
    "# print(train_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# print(train_df)\n",
    "# print(train_df.id.values)\n",
    "# def datasampler(len, percentage):\n",
    "file_idx = list(range(train_df.shape[0]))\n",
    "train_idx, val_idx = train_test_split(file_idx, test_size=0.2, shuffle=True)\n",
    "# print(val_idx.shape)\n",
    "train_dl = DataLoader(train_df, batch_size=batch_size, sampler=train_idx)\n",
    "val_dl = DataLoader(train_df, batch_size=batch_size, sampler=val_idx)\n",
    "# train_mask_dl = DataLoader(train_df, batch_size=batch_size, sampler=train_idx)\n",
    "# print(train_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dl.__dict__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def do_kaggle_metric(predict,truth, threshold=0.5):\n",
    "\n",
    "    N = len(predict)\n",
    "    predict = predict.reshape(N,-1)\n",
    "    truth   = truth.reshape(N,-1)\n",
    "\n",
    "    predict = predict>threshold\n",
    "    truth   = truth>0.5\n",
    "    intersection = truth & predict\n",
    "    union        = truth | predict\n",
    "    iou = intersection.sum(1)/(union.sum(1)+1e-8)\n",
    "\n",
    "    #-------------------------------------------\n",
    "    result = []\n",
    "    precision = []\n",
    "    is_empty_truth   = (truth.sum(1)==0)\n",
    "    is_empty_predict = (predict.sum(1)==0)\n",
    "\n",
    "    threshold = np.array([0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95])\n",
    "    for t in threshold:\n",
    "        p = iou>=t\n",
    "\n",
    "        tp  = (~is_empty_truth)  & (~is_empty_predict) & (iou> t)\n",
    "        fp  = (~is_empty_truth)  & (~is_empty_predict) & (iou<=t)\n",
    "        fn  = (~is_empty_truth)  & ( is_empty_predict)\n",
    "        fp_empty = ( is_empty_truth)  & (~is_empty_predict)\n",
    "        tn_empty = ( is_empty_truth)  & ( is_empty_predict)\n",
    "\n",
    "        p = (tp + tn_empty) / (tp + tn_empty + fp + fp_empty + fn)\n",
    "\n",
    "        result.append( np.column_stack((tp,fp,fn,tn_empty,fp_empty)) )\n",
    "        precision.append(p)\n",
    "\n",
    "    result = np.array(result).transpose(1,2,0)\n",
    "    precision = np.column_stack(precision)\n",
    "    precision = precision.mean(1)\n",
    "\n",
    "    return precision, result, threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
