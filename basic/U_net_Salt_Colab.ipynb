{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T12:07:26.038986Z",
     "end_time": "2023-04-13T12:07:29.456620Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T12:07:29.462395Z",
     "end_time": "2023-04-13T12:07:29.464522Z"
    }
   },
   "outputs": [],
   "source": [
    "n_fold = 5  # ?\n",
    "pad_left = 0\n",
    "pad_right = 0\n",
    "fine_size = 202 # ?\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "snapshot = 6 # ? what for\n",
    "max_lr = 0.012\n",
    "min_lr = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "save_weight = '../rst/'\n",
    "\n",
    "# if not os.path.isdir(save_weight):\n",
    "#     os.makedirs(save_weight)\n",
    "weight_name = 'model_' + str(fine_size + pad_left + pad_right)\n",
    "\n",
    "train_image_path = './data/SaltDataset/train/images'\n",
    "train_mask_path = './data/SaltDataset/train/masks'\n",
    "test_image_path = './data/SaltDataset/test/images'\n",
    "train_file = './data/SaltDataset/train.csv'\n",
    "depths_file = './data/SaltDataset/depths.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image --[GAN get latent]--> use the depth --[get feature map latent]-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T12:07:29.545614Z",
     "end_time": "2023-04-13T12:07:29.548763Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "        self.encoder = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # # nn.MaxPool2d(2, 2),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img = self.conv1(img)\n",
    "        out = self.encoder(img)\n",
    "        print(img.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.img_shape = (((img_shape // 2) // 2) // 2) // 2\n",
    "        self.linear = nn.Linear(self.img_shape ^2 * 256, 128)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        rst = self.conv(img)\n",
    "        rst = rst.view(img.shape[0], -1)\n",
    "        rst = self.linear(rst)\n",
    "        rst = self.fc(rst)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T12:07:29.555538Z",
     "end_time": "2023-04-13T12:07:29.590428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "4000"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depths = pd.read_csv('./data/SaltDataset/depths.csv')\n",
    "# depths.sort_values('z', inplace=True)\n",
    "# depths.drop('z', axis=1, inplace=True)\n",
    "# depths['fold'] = (list(range(0,5)) * depths.shape[0])[:depths.shape[0]]\n",
    "#\n",
    "train_df = pd.read_csv('./data/SaltDataset/train.csv')\n",
    "len(train_df)\n",
    "# train_df = train_df.merge(depths)\n",
    "# dist = []\n",
    "# for id in train_df.id.values:\n",
    "#   img = cv2.imread(f'./data/SaltDataset/train/images/{id}.png', cv2.IMREAD_GRAYSCALE)\n",
    "#   dist.append(np.unique(img).shape[0])\n",
    "# train_df['unique_pixels'] = dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-13T12:07:29.595688Z",
     "end_time": "2023-04-13T12:07:31.233259Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SaltDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, train_file, depth_file, mode='train', transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.train_df = pd.read_csv(train_file).drop('rle_mask', axis=1)\n",
    "        self.depth_df = pd.read_csv(depth_file)\n",
    "        self.train_df = self.train_df.merge(self.depth_df)\n",
    "        self.transform = transform\n",
    "        # self.mode = mode\n",
    "        # if mode == 'train':\n",
    "        #     self.file_idx = list(range(len(self.train_df)))\n",
    "        #     self.train_idx, self.val_idx = train_test_split(file_idx, test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # if self.train_idx:\n",
    "        #     pass\n",
    "        # elif self.mode == 'val':\n",
    "        #     pass\n",
    "        img_path = os.path.join(self.img_path, self.train_df.values[idx, 0] + '.png')\n",
    "        mask_path = os.path.join(self.mask_path, self.train_df.values[idx, 0] + '.png')\n",
    "        depth = self.train_df.values[idx, 1]\n",
    "\n",
    "        # image = read_image(img_path) # , mode=ImageReadMode.GRAY\n",
    "        # mask = read_image(mask_path)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "            # depth = self.transform(depth)\n",
    "        depth = torch.tensor([[depth]], dtype=torch.float32)\n",
    "\n",
    "        depth = torch.unsqueeze(depth, 0)\n",
    "        # print(depth.shape)\n",
    "        # depth = torch.unsqueeze(depth, 0)\n",
    "\n",
    "        return image, mask, depth\n",
    "\n",
    "\n",
    "# def trainImageFetch(image_len):\n",
    "#     image_train = np.zeros((image_len, 101, 101), dtype=np.float32) / 255\n",
    "#     mask_train = np.zeros((image_len, 101, 101), dtype=np.float32) / 255\n",
    "\n",
    "# depths = pd.read_csv('./data/SaltDataset/depths.csv')\n",
    "# train_df = pd.read_csv('./data/SaltDataset/train.csv')\n",
    "# train_df = train_df.merge(depths)       # merge 就像数据库一样合并表，主键自动匹配\n",
    "\n",
    "# for id in train_df.id.values:\n",
    "#     img = cv2.imread(f'./data/SaltDataset/train/images/{id}.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     dist.append(np.unique(img).shape[0])\n",
    "#     # print(img.shape)\n",
    "#\n",
    "#\n",
    "# train_df['unique_pixels'] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataset = SaltDataset(train_image_path, train_mask_path, train_file, depths_file, transform=transforms.ToTensor())\n",
    "\n",
    "file_idx = list(range(train_df.shape[0]))\n",
    "train_idx, val_idx = train_test_split(file_idx, test_size=0.2, shuffle=True)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "# print(training_data.__dict__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T12:07:31.233634Z",
     "end_time": "2023-04-13T12:07:31.285896Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# model\n",
    "g_model = Generator().to(device)\n",
    "d_model = Discriminator(img_shape=101).to(device)\n",
    "\n",
    "# optimzer\n",
    "G_optim = optim.SGD(g_model.parameters(), lr=min_lr, momentum=0.05)\n",
    "D_optim = optim.SGD(d_model.parameters(), lr=min_lr, momentum=0.05)\n",
    "\n",
    "# loss\n",
    "g_loss = nn.MSELoss()\n",
    "d_loss = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T12:07:31.289209Z",
     "end_time": "2023-04-13T12:07:31.299738Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## U-net intro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from U_net import Contract, Unet\n",
    "unet = Unet(1, pad=True).to(device)\n",
    "U_optim = optim.SGD(unet.parameters(), lr=min_lr, momentum=0.05)\n",
    "u_loss = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T12:07:31.302233Z",
     "end_time": "2023-04-13T12:07:31.483307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Contract' object has no attribute 'pad'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 28\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m idx, (imgs, masks, depths) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader, start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;66;03m# # print(imgs), print(masks), print(depths)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;66;03m# imgs = imgs.to(device)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;66;03m# print(f\"Pred_depth: {pred_depth}\\tloss_a: {loss_a}\\tloss_b: {loss_b}\\tTotal Loss: {loss}\")\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m         preds \u001B[38;5;241m=\u001B[39m \u001B[43munet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m         loss \u001B[38;5;241m=\u001B[39m u_loss(preds, masks)\n\u001B[1;32m     31\u001B[0m         U_optim\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/CV_tinyproject/learning/basic/U_net.py:84\u001B[0m, in \u001B[0;36mUnet.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m---> 84\u001B[0m     out, l1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     out, l2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(out)\n\u001B[1;32m     86\u001B[0m     out, l3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(out)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/CV_tinyproject/learning/basic/U_net.py:24\u001B[0m, in \u001B[0;36mContract.forward\u001B[0;34m(self, x, last)\u001B[0m\n\u001B[1;32m     22\u001B[0m width \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m     23\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x)\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad\u001B[49m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m     25\u001B[0m     out \u001B[38;5;241m=\u001B[39m centerCrop(out, height, width)\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;66;03m# pass\u001B[39;00m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;66;03m# output should be cropped here\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1207\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1205\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1206\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1207\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1208\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Contract' object has no attribute 'pad'"
     ]
    }
   ],
   "source": [
    "# print(train_loader.dataset.__getitem__(0))\n",
    "\n",
    "def train(dataloader, model, epochs):\n",
    "    pass\n",
    "\n",
    "# train discriminator\n",
    "for epoch in range(epochs):\n",
    "    for idx, (imgs, masks, depths) in enumerate(train_loader, start=0):\n",
    "        # # print(imgs), print(masks), print(depths)\n",
    "        # imgs = imgs.to(device)\n",
    "        # # masks = masks.to(device)\n",
    "        # pred_feature = g_model(depths)\n",
    "        # pred_depth = d_model(pred_feature)\n",
    "        # loss_a = d_loss(pred_depth, depths)\n",
    "        # loss_b = d_loss(imgs, depths)\n",
    "        # loss = (loss_a + loss_b) / 2\n",
    "        #\n",
    "        # # optimizer.zero\n",
    "        # G_optim.zero_grad()\n",
    "        # D_optim.zero_grad()\n",
    "        #\n",
    "        # # backprop\n",
    "        # loss.backward()\n",
    "        # D_optim.step()\n",
    "        # G_optim.step()\n",
    "        #\n",
    "        # print(f\"Pred_depth: {pred_depth}\\tloss_a: {loss_a}\\tloss_b: {loss_b}\\tTotal Loss: {loss}\")\n",
    "        preds = unet(imgs)\n",
    "        loss = u_loss(preds, masks)\n",
    "\n",
    "        U_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        U_optim.step()\n",
    "\n",
    "        print(f\"loss: {loss}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T21:31:18.337936Z",
     "end_time": "2023-04-07T21:31:18.349343Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-12T12:47:38.108291Z",
     "end_time": "2023-04-12T12:47:38.117856Z"
    }
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "mask_list = []\n",
    "# for _, id in tqdm(enumerate(train_df.id.values), total=len(train_df)):\n",
    "#     # print(id)\n",
    "#     img = cv2.imread(f'./data/SaltDataset/train/images/{id}.png', cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "#     mask = cv2.imread(f'./data/SaltDataset/train/masks/{id}.png', cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "#     # print(img)\n",
    "#     img_list.append(img)\n",
    "#     mask_list.append(mask)\n",
    "#\n",
    "# print(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T14:15:03.102041Z",
     "start_time": "2023-04-06T14:15:03.094510Z"
    }
   },
   "outputs": [],
   "source": [
    "print(mask_list)\n",
    "# print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:52:42.562129Z",
     "start_time": "2023-04-06T15:52:42.548202Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T14:15:59.971205Z",
     "start_time": "2023-04-06T14:15:58.814983Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "# train_data = np.concatenate((img_list, mask_list), axis=0)\n",
    "# train_data = pd.DataFrame(data=img_list)\n",
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T14:16:01.445690Z",
     "start_time": "2023-04-06T14:16:01.442092Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# print(train_df)\n",
    "# print(train_df.id.values)\n",
    "# def datasampler(len, percentage):\n",
    "file_idx = list(range(train_df.shape[0]))\n",
    "train_idx, val_idx = train_test_split(file_idx, test_size=0.2, shuffle=True)\n",
    "# print(val_idx.shape)\n",
    "train_dl = DataLoader(train_df, batch_size=batch_size, sampler=train_idx)\n",
    "val_dl = DataLoader(train_df, batch_size=batch_size, sampler=val_idx)\n",
    "# train_mask_dl = DataLoader(train_df, batch_size=batch_size, sampler=train_idx)\n",
    "# print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:52:51.881392Z",
     "start_time": "2023-04-06T15:52:51.873773Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dl.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T20:27:23.139663Z",
     "start_time": "2023-04-05T20:27:23.120288Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_kaggle_metric(predict,truth, threshold=0.5):\n",
    "\n",
    "    N = len(predict)\n",
    "    predict = predict.reshape(N,-1)\n",
    "    truth   = truth.reshape(N,-1)\n",
    "\n",
    "    predict = predict>threshold\n",
    "    truth   = truth>0.5\n",
    "    intersection = truth & predict\n",
    "    union        = truth | predict\n",
    "    iou = intersection.sum(1)/(union.sum(1)+1e-8)\n",
    "\n",
    "    #-------------------------------------------\n",
    "    result = []\n",
    "    precision = []\n",
    "    is_empty_truth   = (truth.sum(1)==0)\n",
    "    is_empty_predict = (predict.sum(1)==0)\n",
    "\n",
    "    threshold = np.array([0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95])\n",
    "    for t in threshold:\n",
    "        p = iou>=t\n",
    "\n",
    "        tp  = (~is_empty_truth)  & (~is_empty_predict) & (iou> t)\n",
    "        fp  = (~is_empty_truth)  & (~is_empty_predict) & (iou<=t)\n",
    "        fn  = (~is_empty_truth)  & ( is_empty_predict)\n",
    "        fp_empty = ( is_empty_truth)  & (~is_empty_predict)\n",
    "        tn_empty = ( is_empty_truth)  & ( is_empty_predict)\n",
    "\n",
    "        p = (tp + tn_empty) / (tp + tn_empty + fp + fp_empty + fn)\n",
    "\n",
    "        result.append( np.column_stack((tp,fp,fn,tn_empty,fp_empty)) )\n",
    "        precision.append(p)\n",
    "\n",
    "    result = np.array(result).transpose(1,2,0)\n",
    "    precision = np.column_stack(precision)\n",
    "    precision = precision.mean(1)\n",
    "\n",
    "    return precision, result, threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
