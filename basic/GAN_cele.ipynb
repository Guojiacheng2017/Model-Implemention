{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel  # as parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as visionUtils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "# ---------------------------------------- #\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7ff2a7b72390>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed = 999\n",
    "manualSeed = 999  # we can set randomly or manually  {manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Hyper-Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataroot = './data/celeba'\n",
    "os.makedirs(dataroot, exist_ok=True)\n",
    "workers = 2     # num of workers for dataloaders ???\n",
    "\n",
    "batch_size = 128\n",
    "image_size = 64  # depend on how large the image is\n",
    "in_channels = 3  # depend on how mang channels of the image (r, g, b)\n",
    "latent = 100    # size of latent vector (z)\n",
    "gen_fea = 64    # size of feature map in generator\n",
    "dis_fea = 64    # **** in discriminator\n",
    "num_epochs = 5  # training epochs\n",
    "lr = 0.002     # optimizer learning rate for \"Adam\"\n",
    "beta1 = 0.5     # beta1 for adam\n",
    "n_gpu = 0       # no GPU using"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 202599\n",
      "    Root location: ./data/celeba\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=64, interpolation=bilinear, max_size=None, antialias=None)\n",
      "               CenterCrop(size=(64, 64))\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           ) 202599 1583\n"
     ]
    }
   ],
   "source": [
    "# download the dataset cele_a or load the dataset\n",
    "cele_a = dset.ImageFolder(root=dataroot,\n",
    "                     transform=(\n",
    "                         transforms.Compose([\n",
    "                             transforms.Resize(image_size),\n",
    "                             transforms.CenterCrop(image_size),\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=False)\n",
    "                             # here we don't use the \"inplace = True\" is because the transforms is assigned to cele_a, which create a new one instead of making changes on the original dataset\n",
    "                         ])\n",
    "                     ),\n",
    "                          ) # dset.CelebA(..., download=True)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = data.DataLoader(cele_a, batch_size=batch_size, shuffle=True, sampler=None, num_workers=workers)    # pin_memory = False  because there 's no GPU on this computer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # cuda or cpu\n",
    "\n",
    "\n",
    "print(dataloader.dataset, len(cele_a), len(dataloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# print(len(cele_a), len(dataloader))\n",
    "# print(len(cele_a.classes))  # only 1 class in this dataset\n",
    "# print(type(cele_a[0][0]))\n",
    "# print(dataloader.__dict__)\n",
    "# print(dataloader.__doc__)\n",
    "# print(type(dataloader),\n",
    "#       type(dataloader.dataset[0]),\n",
    "#       type(dataloader.dataset[0][0]))\n",
    "\n",
    "# img0 = dataloader.dataset[0][0]\n",
    "# print(img0.device)\n",
    "# a = next(iter(dataloader))\n",
    "\n",
    "\n",
    "# plt.figure()    # figsize=(8, 8)\n",
    "# plt.imshow(np.transpose(visionUtils.make_grid(img0, padding=0, normalize=True).cpu(), (1,2,0)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# # print(a[0][:64])\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis('off')\n",
    "#\n",
    "# # normalize back to the image\n",
    "# # 1.\n",
    "# img = a[0][0] * 128 + 128\n",
    "# plt.imshow(np.transpose(img / 255, axes=(1, 2, 0)))\n",
    "\n",
    "# 2.\n",
    "# plt.imshow(np.transpose(visionUtils.make_grid(a[0][:64], padding=0, normalize=True), axes=(1, 2, 0)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Original Code for normalize the images to imshows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Plot some training images\n",
    "# real_batch = next(iter(dataloader))\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(visionUtils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# print(type(real_batch[0]), len(real_batch[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weight Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# custom weight initialization on netG and netD\n",
    "# what for ??? why this format ???\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generator\n",
    "<img src=\"../img_indoc/dcgan_generator.png\" alt=\"generator model\" width=\"700\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# ![generator model](../img_indoc/dcgan_generator.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Discriminator(\n  (backbone): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (9): Sigmoid()\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import Generator, Discriminator\n",
    "net_G = Generator(latent, gen_fea, in_channels, num_gpu=n_gpu).to(device)  # generate in_channels size imgs\n",
    "net_D = Discriminator(gen_fea=gen_fea, channels=in_channels, num_gpu=n_gpu).to(device) # distinguish images in in_channels size\n",
    "\n",
    "# multi-gpu if desired (Data Parallel, not model parallel or )\n",
    "if (device.type == \"cuda\") and (n_gpu > 1):\n",
    "    net_G = nn.DataParallel(net_G, list(range(n_gpu)))\n",
    "    net_D = nn.DataParallel(net_D, list(range(n_gpu)))\n",
    "\n",
    "net_G.apply(weights_init)\n",
    "net_D.apply(weights_init)\n",
    "\n",
    "# print(net_G, \"\\n\", net_D)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function and Optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() # binary cross-entropy loss function\n",
    "# loss = -y * log(D(x)) - (1 - y) * log(1 - D(G(z))) where y is label, D(x) is prediction of real images and D(G(z)) is prediction of fake images\n",
    "# loss of generator => maximize the log(D(G(z)) to deceive the Discriminator the output is the real image\n",
    "#       close to 0.5 is best, because that means the discriminator(which is also learn in this process) cannot tell which image is real or fake\n",
    "\n",
    "fixed_noise = torch.randn(64, latent, 1, 1, device=device)\n",
    "real_label = 1; fake_label = 0;\n",
    "\n",
    "# Setup Optimizer\n",
    "optim_G = optim.Adam(net_G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optim_D = optim.Adam(net_D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "# optim_G = optim.SGD(net_G.parameters(), lr=lr, momentum=0.1)\n",
    "# optim_D = optim.SGD(net_D.parameters(), lr=lr, momentum=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Looping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/bling/opt/anaconda3/envs/cv/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/bling/opt/anaconda3/envs/cv/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/Users/bling/opt/anaconda3/envs/cv/lib/python3.9/site-packages/torch/__init__.py\", line 711, in <module>\n",
      "    from torch import hub as hub\n",
      "  File \"/Users/bling/opt/anaconda3/envs/cv/lib/python3.9/site-packages/torch/hub.py\", line 18, in <module>\n",
      "    from tqdm.auto import tqdm  # automatically select proper tqdm submodule if available\n",
      "  File \"/Users/bling/opt/anaconda3/envs/cv/lib/python3.9/site-packages/tqdm/__init__.py\", line 3, in <module>\n",
      "    from .cli import main  # TODO: remove in v5.0.0\n",
      "  File \"/Users/bling/opt/anaconda3/envs/cv/lib/python3.9/site-packages/tqdm/cli.py\", line 9, in <module>\n",
      "    from .std import TqdmKeyError, TqdmTypeError, tqdm\n",
      "  File \"/Users/bling/opt/anaconda3/envs/cv/lib/python3.9/site-packages/tqdm/std.py\", line 22, in <module>\n",
      "    from .utils import (\n",
      "  File \"/Users/bling/opt/anaconda3/envs/cv/lib/python3.9/site-packages/tqdm/utils.py\", line 325, in <module>\n",
      "    from unicodedata import east_asian_width\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/t4/vlfl8bvn5mqgcfzprpqf_zph0000gn/T/ipykernel_31228/3172302902.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;31m# For each batch in the dataloader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m         \u001B[0;31m# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mnet_D\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    357\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_iterator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    358\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 359\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    360\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    361\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    303\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_worker_number_rationality\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 305\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0m_MultiProcessingDataLoaderIter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m    916\u001B[0m             \u001B[0;31m#     before it starts, and __del__ tries to join but will get:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    917\u001B[0m             \u001B[0;31m#     AssertionError: can only join a started process.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 918\u001B[0;31m             \u001B[0mw\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    919\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_index_queues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex_queue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    920\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_workers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/multiprocessing/process.py\u001B[0m in \u001B[0;36mstart\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    119\u001B[0m                \u001B[0;34m'daemonic processes are not allowed to have children'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m         \u001B[0m_cleanup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 121\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_popen\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_Popen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    122\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sentinel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_popen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msentinel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0;31m# Avoid a refcycle if the target function holds an indirect\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/multiprocessing/context.py\u001B[0m in \u001B[0;36m_Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    222\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_Popen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 224\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_default_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mProcess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_Popen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    225\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mDefaultContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mBaseContext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/multiprocessing/context.py\u001B[0m in \u001B[0;36m_Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0m_Popen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mpopen_spawn_posix\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPopen\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m     \u001B[0;32mclass\u001B[0m \u001B[0mForkServerProcess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBaseProcess\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprocess_obj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mduplicate_for_child\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/multiprocessing/popen_fork.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreturncode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinalizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_launch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mduplicate_for_child\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/cv/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001B[0m in \u001B[0;36m_launch\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     60\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msentinel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparent_r\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparent_w\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wb'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclosefd\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 62\u001B[0;31m                 \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetbuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     63\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m             \u001B[0mfds_to_close\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Looping...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, start=0):\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        net_D.zero_grad()\n",
    "\n",
    "        #format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size, ), real_label, dtype=torch.float, device=device)\n",
    "        # forward pass real batch through D\n",
    "        output = net_D(real_cpu).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        noise = torch.randn(b_size, latent, 1, 1, dtype=torch.float, device=device)\n",
    "        fake = net_G(noise)     # prediction for fake images\n",
    "        # print(noise.shape, fake.shape)\n",
    "        label.fill_(fake_label) # change label data from 1 -> 0\n",
    "        output = net_D(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = (errD_fake + errD_real) / 2\n",
    "        # print(errD_real.detach(), errD_fake.detach())\n",
    "\n",
    "        optim_D.step()\n",
    "\n",
    "\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        net_G.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = net_D(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "\n",
    "        optim_G.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d] [%d/%d]\\t loss_d: %.4f\\t loss_g: %.4f\\t D(x): %.4f\\t D(G(z)): %.4f/%.4f' % (epoch, num_epochs, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
    "            with torch.no_grad():\n",
    "                fake = net_G(fixed_noise).detach().cpu()\n",
    "            img_list.append(visionUtils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(visionUtils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
